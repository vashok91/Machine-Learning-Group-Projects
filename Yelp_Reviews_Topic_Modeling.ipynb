{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark to initialize spark session in notebook\n",
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark packages\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, count, explode\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "|         business_id|                name|       stars|review_count|is_open|          categories|\n",
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "|FYWN1wneV18bWNgQj...|\"\"\"Dental by Desi...|-111.9785992|         4.0|     22|                   1|\n",
      "|He-G7vWjzVUysIKrf...|\"\"\"Stephen Szabo ...|         3.0|          11|      1|Hair Stylists;Hai...|\n",
      "|KQPW8lFf1y5BT2Mxi...|\"\"\"Western Motor ...|-112.1153098|         1.5|     18|                   1|\n",
      "|8DShNS-LuFqpEWIp0...|\"\"\"Sports Authori...|-111.9647254|         3.0|      9|                   0|\n",
      "|PfOCPjBrlQAnz__NX...|\"\"\"Brick House Ta...|         3.5|         116|      1|American (New);Ni...|\n",
      "|o9eMRCWt5PkpLDE0g...|       \"\"\"Messina\"\"\"|         4.0|           5|      1| Italian;Restaurants|\n",
      "|kCoE3jvEtg6UVz5SO...|    \"\"\"BDJ Realty\"\"\"|  -115.26846|         4.0|      5|                   1|\n",
      "|OD2hnuuTJI9uotcKy...|   \"\"\"Soccer Zone\"\"\"|-115.2496601|         1.5|      9|                   1|\n",
      "|EsMcGiZaQuG1OOvL9...|\"\"\"Any Given Sund...|         5.0|          15|      1|Coffee & Tea;Ice ...|\n",
      "|TGWhGNusxyMaA4kQV...|\"\"\"Detailing Gone...|         5.0|           7|      1|Automotive;Auto D...|\n",
      "|XOSRcvtaKc_Q5H1SA...|\"\"\"East Coast Cof...|         4.5|           3|      0|Breakfast & Brunc...|\n",
      "|Y0eMNa5C-YU1RQOZf...|\"\"\"CubeSmart Self...|         5.0|          23|      1|Local Services;Se...|\n",
      "|xcgFnd-MwkZeO5G2H...|\"\"\"T & T Bakery a...|         4.0|          38|      1|Bakeries;Bagels;Food|\n",
      "|NmZtoE3v8RdSJEczY...|\"\"\"Complete Denta...|         2.0|           5|      1|General Dentistry...|\n",
      "|fNMVV_ZX7CJSDWQGd...|\"\"\"Showmars Gover...|         3.5|           7|      1|Restaurants;Ameri...|\n",
      "|l09JfMeQ6ynYs5MCJ...|\"\"\"Alize Catering\"\"\"|         3.0|          12|      0|Italian;French;Re...|\n",
      "|IQSlT5jGE6CCDhSG0...|\"\"\"T & Y Nail Spa\"\"\"|-112.2400118|         3.0|     20|                   1|\n",
      "|b2I2DXtZVnpUMCXp1...|\"\"\"Meineke Car Ca...|         3.5|           9|      1|Tires;Oil Change ...|\n",
      "|0FMKDOU8TJT1x87OK...|\"\"\"Senior's Barbe...|         5.0|          65|      1|Barbers;Beauty & ...|\n",
      "|Gu-xs3NIQTj3Mj2xY...|\"\"\"Maxim Bakery &...| -79.4126618|         3.5|     34|                   1|\n",
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174567"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yelp Business Dataset\n",
    "yelp_business_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\",\"true\").load(r\"C:\\Users\\vaibh\\MSA 8050\\FinalProject\\yelp_business.csv\")\n",
    "yelp_business_df2 = yelp_business_df.select(\"business_id\", \"name\", \"stars\", \"review_count\", \"is_open\", \"categories\")\n",
    "yelp_business_df2.show()\n",
    "yelp_business_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "|         business_id|                name|       stars|review_count|is_open|          categories|\n",
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "|FYWN1wneV18bWNgQj...|\"\"\"Dental by Desi...|-111.9785992|         4.0|     22|                   1|\n",
      "|He-G7vWjzVUysIKrf...|\"\"\"Stephen Szabo ...|         3.0|          11|      1|Hair Stylists;Hai...|\n",
      "|KQPW8lFf1y5BT2Mxi...|\"\"\"Western Motor ...|-112.1153098|         1.5|     18|                   1|\n",
      "|8DShNS-LuFqpEWIp0...|\"\"\"Sports Authori...|-111.9647254|         3.0|      9|                   0|\n",
      "|PfOCPjBrlQAnz__NX...|\"\"\"Brick House Ta...|         3.5|         116|      1|American (New);Ni...|\n",
      "|o9eMRCWt5PkpLDE0g...|       \"\"\"Messina\"\"\"|         4.0|           5|      1| Italian;Restaurants|\n",
      "|kCoE3jvEtg6UVz5SO...|    \"\"\"BDJ Realty\"\"\"|  -115.26846|         4.0|      5|                   1|\n",
      "|OD2hnuuTJI9uotcKy...|   \"\"\"Soccer Zone\"\"\"|-115.2496601|         1.5|      9|                   1|\n",
      "|EsMcGiZaQuG1OOvL9...|\"\"\"Any Given Sund...|         5.0|          15|      1|Coffee & Tea;Ice ...|\n",
      "|TGWhGNusxyMaA4kQV...|\"\"\"Detailing Gone...|         5.0|           7|      1|Automotive;Auto D...|\n",
      "|XOSRcvtaKc_Q5H1SA...|\"\"\"East Coast Cof...|         4.5|           3|      0|Breakfast & Brunc...|\n",
      "|Y0eMNa5C-YU1RQOZf...|\"\"\"CubeSmart Self...|         5.0|          23|      1|Local Services;Se...|\n",
      "|xcgFnd-MwkZeO5G2H...|\"\"\"T & T Bakery a...|         4.0|          38|      1|Bakeries;Bagels;Food|\n",
      "|NmZtoE3v8RdSJEczY...|\"\"\"Complete Denta...|         2.0|           5|      1|General Dentistry...|\n",
      "|fNMVV_ZX7CJSDWQGd...|\"\"\"Showmars Gover...|         3.5|           7|      1|Restaurants;Ameri...|\n",
      "|l09JfMeQ6ynYs5MCJ...|\"\"\"Alize Catering\"\"\"|         3.0|          12|      0|Italian;French;Re...|\n",
      "|IQSlT5jGE6CCDhSG0...|\"\"\"T & Y Nail Spa\"\"\"|-112.2400118|         3.0|     20|                   1|\n",
      "|b2I2DXtZVnpUMCXp1...|\"\"\"Meineke Car Ca...|         3.5|           9|      1|Tires;Oil Change ...|\n",
      "|0FMKDOU8TJT1x87OK...|\"\"\"Senior's Barbe...|         5.0|          65|      1|Barbers;Beauty & ...|\n",
      "|Gu-xs3NIQTj3Mj2xY...|\"\"\"Maxim Bakery &...| -79.4126618|         3.5|     34|                   1|\n",
      "+--------------------+--------------------+------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- review_count: string (nullable = true)\n",
      " |-- is_open: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the dataset\n",
    "yelp_business_df3 = yelp_business_df2.dropna(subset=[\"business_id\", \"name\", \"stars\", \"review_count\", \"is_open\", \"categories\"])\n",
    "yelp_business_df3.show()\n",
    "yelp_business_df3.printSchema()\n",
    "yelp_business_df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+------------+-------+--------------------+\n",
      "|         business_id|                name|stars|review_count|is_open|          categories|\n",
      "+--------------------+--------------------+-----+------------+-------+--------------------+\n",
      "|SkjO4uWLRg4NF198y...|     \"\"\"Pizzaiolo\"\"\"|  1.0|           3|      1|Pizza;Gluten-Free...|\n",
      "|RScKS72p6_AykRRwZ...|\"\"\"Roni's Pizzeri...|  1.0|           5|      0|   Pizza;Restaurants|\n",
      "|OPdjBa5toeaVsDleg...|\"\"\"Authentic Pizz...|  1.0|           3|      0|   Restaurants;Pizza|\n",
      "|kgAFhUm4sim7RPnND...|\"\"\"Papa John's Pi...|  1.0|           7|      1|   Pizza;Restaurants|\n",
      "|0dXCoyvcvW3Lx1D8O...| \"\"\"Le Bistro 426\"\"\"|  1.0|           3|      1| Restaurants;Italian|\n",
      "|FO2SNKF9I8mvTCAvb...| \"\"\"Pizza Palermo\"\"\"|  1.0|           4|      1|   Restaurants;Pizza|\n",
      "|QPpptKYmU3fZMi_Js...|   \"\"\"Pizza Depot\"\"\"|  1.0|           4|      1|   Restaurants;Pizza|\n",
      "|kkvhQBT1Oh73sVSYZ...|    \"\"\"McDonald's\"\"\"|  1.0|           8|      1|Burgers;Fast Food...|\n",
      "|Px6FNVLlN9wZQ-uad...|\"\"\"Burger King Re...|  1.0|           5|      1| Burgers;Restaurants|\n",
      "|H1LhTQWW_YCgJejzB...|\"\"\"Little Caesars...|  1.0|           3|      1|   Restaurants;Pizza|\n",
      "|JtqTHVL6yYvBrm25U...|\"\"\"Zingers Sports...|  1.0|           3|      1|Nightlife;Restaur...|\n",
      "|I79WiJ-7Xkufc392q...|   \"\"\"Burger King\"\"\"|  1.0|           4|      1|Burgers;Restauran...|\n",
      "|EPKn6OQrMPs8zVlad...|           \"\"\"A&W\"\"\"|  1.0|           3|      1|Burgers;Breakfast...|\n",
      "|nd-dSwGuSGhx-fF3-...|\"\"\"Oak Ridges Asi...|  1.0|           3|      1| Restaurants;Chinese|\n",
      "|2vuAlukfUZf3vbHse...|    \"\"\"McDonald's\"\"\"|  1.0|           9|      1|Fast Food;Restaur...|\n",
      "|IyrwvRwiwsKCiiKox...|   \"\"\"Noor Makkah\"\"\"|  1.0|           3|      1|  Indian;Restaurants|\n",
      "|8i04GQgUY3g8q2iw7...|    \"\"\"McDonald's\"\"\"|  1.0|           4|      1|Restaurants;Fast ...|\n",
      "|E4U8RCe42CpT3rtof...|       \"\"\"Wendy's\"\"\"|  1.0|           5|      1|Burgers;Fast Food...|\n",
      "|n9mxpYKigTM6llGAC...|    \"\"\"McDonald's\"\"\"|  1.0|           6|      1|Burgers;Restauran...|\n",
      "|mSbRhODRzl51Qu8Hv...|\"\"\"Broastyy Fried...|  1.0|           3|      0|Restaurants;Chick...|\n",
      "+--------------------+--------------------+-----+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering Data\n",
    "yelp_business_df4 = yelp_business_df3.filter(yelp_business_df3.stars.isin('1.0', '1.5', '2.0', '2.5', '3.0', '3.5', \n",
    "                                                                          '4.0', '4.5', '5.0'))\n",
    "yelp_business_df5 = yelp_business_df4.filter(yelp_business_df4.categories.like('%Restaurants%'))\n",
    "\n",
    "yelp_business_df5 = yelp_business_df5.orderBy('stars', ascending=True)\n",
    "yelp_business_df5.show()\n",
    "yelp_business_df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data to csv for viewing purposes\n",
    "yelp_business_df5.write.csv(r\"C:\\Users\\vaibh\\MSA 8050\\FinalProject\\yelp_business_test6.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|         business_id|stars|                text|\n",
      "+--------------------+-----+--------------------+\n",
      "|AEx2SYEUJmTxVVB18...|    5|Super simple plac...|\n",
      "|VR6GpWIda3SfvPC-l...|    5|Small unassuming ...|\n",
      "|CKC0-MOWMqoeWf6s-...|    5|Lester's is locat...|\n",
      "|ACFtxLv8pGrrxMm6E...|    4|Love coming here....|\n",
      "|s2I_Ni76bjJNK9yG6...|    4|Had their chocola...|\n",
      "|8QWPlVQ6D-OExqXoa...|    5|Cycle Pub Las Veg...|\n",
      "|9_CGhHMz8698M9-Pk...|    4|Who would have gu...|\n",
      "|gkCorLgPyQLsptTHa...|    4|Always drove past...|\n",
      "|5r6-G9C4YLbC7Ziz5...|    3|Not bad!! Love th...|\n",
      "|fDF_o2JPU8BR1Gya-...|    5|Love this place!\n",
      "...|\n",
      "|z8oIoCT1cXz7gZP5G...|    4|This is currently...|\n",
      "|XWTPNfskXoUL-Lf32...|    3|Server was a litt...|\n",
      "|13nKUHH-uEUXVZylg...|    1|I thought Tidy's ...|\n",
      "|RtUvSWO_UZ8V3Wpj0...|    3|Wanted to check o...|\n",
      "|Aov96CM4FZAXeZvKt...|    5|This place is awe...|\n",
      "|0W4lkclzZThpx3V65...|    4|a must stop when ...|\n",
      "|fdnNZMk1NP7ZhL-YM...|    1|I too have been t...|\n",
      "|PFPUMF38-lraKzLcT...|    3|Came here with my...|\n",
      "|oWTn2IzrprsRkPfUL...|    3|Came here for a b...|\n",
      "|zgQHtqX0gqMw1nlBZ...|    1|really excited to...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yelp Review Dataset\n",
    "yelp_review_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\",\"true\").load(r\"C:\\Users\\vaibh\\MSA 8050\\FinalProject\\yelp_review.csv\")\n",
    "yelp_review_df2 = yelp_review_df.select(\"business_id\", \"stars\", \"text\")\n",
    "yelp_review_df2.show()\n",
    "yelp_review_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|         business_id|label|                text|\n",
      "+--------------------+-----+--------------------+\n",
      "|0Rni7ocMC_Lg2UH0l...|  1.0|This place is alw...|\n",
      "|YvuniBBiSs66vx9Gt...|  1.0|The saga has fini...|\n",
      "|3uBrRcIhbhed1xftL...|  1.0|Horrible customer...|\n",
      "|JlNeaOymdVbE6_bub...|  1.0|We always go to t...|\n",
      "|0g7Pr8OWl_t_7DUeY...|  1.0|I bought a groupo...|\n",
      "|fdnNZMk1NP7ZhL-YM...|  1.0|I too have been t...|\n",
      "|uN2oZDJGO078ExbbV...|  1.0|\"I give up on thi...|\n",
      "|l1_S1mfGbEMxfT1f9...|  1.0|Terrible service ...|\n",
      "|zgQHtqX0gqMw1nlBZ...|  1.0|really excited to...|\n",
      "|DGkjLTH7BiHp_sMgU...|  1.0|\"Worst experience...|\n",
      "|hjk3ox7w1akbEuOgT...|  1.0|Food is very blan...|\n",
      "|806kkDGaRCJ4lZLRc...|  1.0|after reading the...|\n",
      "|OwYElCdiJ1IGKVJ4-...|  1.0|\"This whole place...|\n",
      "|X29BLACjBhOg_P2US...|  1.0|after reading rev...|\n",
      "|aNNOpMame_VEIIZUQ...|  1.0|\"Pandora charms a...|\n",
      "|pLZ9oZM8c6MNbRlg0...|  1.0|\"THEY SELL WRECKE...|\n",
      "|-dy1d0ohs4D8qkJoe...|  1.0|No...I've had it ...|\n",
      "|13nKUHH-uEUXVZylg...|  1.0|I thought Tidy's ...|\n",
      "| or a romantic da...|  1.0|                   1|\n",
      "|S5bNE4Pmin8OQUMOF...|  1.0|Came here for a f...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the dataset\n",
    "yelp_review_df3 = yelp_review_df2.dropna(subset=[\"business_id\", \"stars\", \"text\"])\n",
    "yelp_review_df3 = yelp_review_df3.withColumn(\"label\", yelp_review_df3[\"stars\"].cast(\"double\"))\n",
    "yelp_review_df3 = yelp_review_df3.select(\"business_id\", \"label\", \"text\")\n",
    "\n",
    "# Filter the dataset\n",
    "yelp_review_df4 = yelp_review_df3.filter(yelp_review_df3.label.isin(1.0, 1.5, 2.0, 2.5, 3.0, 3.5, \n",
    "                                                                          4.0, 4.5, 5.0))\n",
    "\n",
    "yelp_review_df4 = yelp_review_df4.orderBy(\"label\", ascending=True)\n",
    "yelp_review_df4.show()\n",
    "yelp_review_df4.printSchema()\n",
    "yelp_review_df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data to csv for viewing purposes\n",
    "yelp_review_df4.write.csv(r\"C:\\Users\\vaibh\\MSA 8050\\FinalProject\\yelp_review_test1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+------------+-------+--------------------+-----+--------------------+\n",
      "|         business_id|                name|stars|review_count|is_open|          categories|label|                text|\n",
      "+--------------------+--------------------+-----+------------+-------+--------------------+-----+--------------------+\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|I brought my two ...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Excellent steakho...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  2.0|I've been to this...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Steak! My first t...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  3.0|Located in the Ve...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Excellent steakho...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|I really love tha...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|I had the bone in...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  1.0|I mainly went for...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|\"Yummy!  I didn't...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Make sure you get...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|We couldn't help ...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Excellent service...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|Well?  I stay at ...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|EXCELLENT!! Every...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|One of my real fi...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|\"My quest for try...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Dinner was just a...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  4.0|Almost perfect ex...|\n",
      "|--9e1ONYQuAa-CB_R...|\"\"\"Delmonico Stea...|  4.0|        1451|      1|Cajun/Creole;Stea...|  5.0|Very big portions...|\n",
      "+--------------------+--------------------+-----+------------+-------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- review_count: string (nullable = true)\n",
      " |-- is_open: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join tables on business_id. I want only to join records matching from both tables\n",
    "result_join_df = yelp_business_df5.join(yelp_review_df4, on=['business_id'], how='inner')\n",
    "result_join_df.show()\n",
    "result_join_df.printSchema()\n",
    "result_join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|  5.0|\n",
      "|  4.0|\n",
      "|  3.0|\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Super simple plac...|  5.0|\n",
      "|Small unassuming ...|  5.0|\n",
      "|Lester's is locat...|  5.0|\n",
      "|Love coming here....|  4.0|\n",
      "|Had their chocola...|  4.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_join_df.select('label').distinct().show()\n",
    "\n",
    "# Choose label and text for topic modeling\n",
    "final_result_df = result_join_df.select('text', 'label')\n",
    "final_result_df.show(5)\n",
    "final_result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split dataset depending on label\n",
    "one_star_df = final_result_df.filter(final_result_df.label.isin(1.0))\n",
    "# one_star_df.show(5)\n",
    "# one_star_df.count()\n",
    "\n",
    "two_star_df = final_result_df.filter(final_result_df.label.isin(2.0))\n",
    "#two_star_df.show(5)\n",
    "# two_star_df.count()\n",
    "\n",
    "three_star_df = final_result_df.filter(final_result_df.label.isin(3.0))\n",
    "#three_star_df.show(5)\n",
    "# three_star_df.count()\n",
    "\n",
    "four_star_df = final_result_df.filter(final_result_df.label.isin(4.0))\n",
    "#four_star_df.show(5)\n",
    "# four_star_df.count()\n",
    "\n",
    "five_star_df = final_result_df.filter(final_result_df.label.isin(5.0))\n",
    "#five_star_df.show(5)\n",
    "# five_star_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing Packages\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re as re\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import monotonically_increasing_id as mi\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Building packages\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and tokenize text\n",
    "def normalize_func(df):\n",
    "    extract_Text = udf(\n",
    "        lambda d: BeautifulSoup(d, \"lxml\").get_text(strip=False), StringType())\n",
    "\n",
    "    remove_Punc = udf(\n",
    "        lambda s: re.sub(r'[^a-zA-Z0-9]|[0-9]', r' ', s).strip().lower(), StringType())\n",
    "\n",
    "    # Remove HTML, puncuations, etc...\n",
    "    normalized_df = df.withColumn(\"text\", remove_Punc(extract_Text(df.text)))\n",
    "    # one_star_normalized_df.show(5)\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\",\n",
    "                               gaps=True, pattern=r'\\s+', minTokenLength=2)\n",
    "    \n",
    "    star_tokens_df = tokenizer.transform(normalized_df)\n",
    "    \n",
    "    # one_star_tokens_df.show(10)\n",
    "    return star_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|food is very blan...|  1.0|[food, is, very, ...|\n",
      "|if you have not y...|  1.0|[if, you, have, n...|\n",
      "|worse customer se...|  1.0|[worse, customer,...|\n",
      "|after reading the...|  1.0|[after, reading, ...|\n",
      "|came here for a f...|  1.0|[came, here, for,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|a few years ago  ...|  2.0|[few, years, ago,...|\n",
      "|atmosphere for th...|  2.0|[atmosphere, for,...|\n",
      "|decided to give t...|  2.0|[decided, to, giv...|\n",
      "|you can see that ...|  2.0|[you, can, see, t...|\n",
      "|i think i may hav...|  2.0|[think, may, have...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|not bad   love th...|  3.0|[not, bad, love, ...|\n",
      "|server was a litt...|  3.0|[server, was, lit...|\n",
      "|wanted to check o...|  3.0|[wanted, to, chec...|\n",
      "|came here with my...|  3.0|[came, here, with...|\n",
      "|came here for a b...|  3.0|[came, here, for,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|love coming here ...|  4.0|[love, coming, he...|\n",
      "|had their chocola...|  4.0|[had, their, choc...|\n",
      "|who would have gu...|  4.0|[who, would, have...|\n",
      "|this is currently...|  4.0|[this, is, curren...|\n",
      "|a must stop when ...|  4.0|[must, stop, when...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|super simple plac...|  5.0|[super, simple, p...|\n",
      "|small unassuming ...|  5.0|[small, unassumin...|\n",
      "|lester s is locat...|  5.0|[lester, is, loca...|\n",
      "|this place is awe...|  5.0|[this, place, is,...|\n",
      "|this place is rea...|  5.0|[this, place, is,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_star_df1 = normalize_func(one_star_df)\n",
    "one_star_df1.show(5)\n",
    "\n",
    "two_star_df1 = normalize_func(two_star_df)\n",
    "two_star_df1.show(5)\n",
    "\n",
    "three_star_df1 = normalize_func(three_star_df)\n",
    "three_star_df1.show(5)\n",
    "\n",
    "four_star_df1 = normalize_func(four_star_df)\n",
    "four_star_df1.show(5)\n",
    "\n",
    "five_star_df1 = normalize_func(five_star_df)\n",
    "five_star_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords and Stemming - One Star Ratings\n",
    "def sw_and_stemming(df):\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    len_english_stopwords = len(english_stopwords)\n",
    "    english_stopwords_ratio = udf(lambda z: len(set(z).intersection(english_stopwords)) / len_english_stopwords)\n",
    "\n",
    "    german_stopwords = set(stopwords.words('german'))\n",
    "    len_german_stopwords = len(german_stopwords)\n",
    "    german_stopwords_ratio = udf(lambda z: len(set(z).intersection(german_stopwords)) / len_german_stopwords)\n",
    "\n",
    "    star_tokens_df_eng = df.withColumn(\"ratio_eng\", english_stopwords_ratio(df['words']))\n",
    "        .withColumn(\"ratio_ger\", german_stopwords_ratio(df['words']))\n",
    "        .withColumn(\"Eng\", col('ratio_eng') > col('ratio_ger')).filter('Eng')\n",
    "    \n",
    "    return star_tokens_df_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|food is very blan...|  1.0|[food, is, very, ...| 0.0670391061452514|0.004310344827586207|true|\n",
      "|if you have not y...|  1.0|[if, you, have, n...|0.13966480446927373|0.004310344827586207|true|\n",
      "|worse customer se...|  1.0|[worse, customer,...| 0.0670391061452514|0.004310344827586207|true|\n",
      "|after reading the...|  1.0|[after, reading, ...| 0.1787709497206704| 0.01293103448275862|true|\n",
      "|came here for a f...|  1.0|[came, here, for,...| 0.2346368715083799|0.017241379310344827|true|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|a few years ago  ...|  2.0|[few, years, ago,...|0.07262569832402235|0.008620689655172414|true|\n",
      "|atmosphere for th...|  2.0|[atmosphere, for,...|0.22346368715083798|0.017241379310344827|true|\n",
      "|decided to give t...|  2.0|[decided, to, giv...| 0.1005586592178771|0.004310344827586207|true|\n",
      "|you can see that ...|  2.0|[you, can, see, t...|0.12849162011173185|0.004310344827586207|true|\n",
      "|i think i may hav...|  2.0|[think, may, have...|0.25139664804469275|0.017241379310344827|true|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|not bad   love th...|  3.0|[not, bad, love, ...|0.08379888268156424|0.004310344827586207|true|\n",
      "|server was a litt...|  3.0|[server, was, lit...| 0.0446927374301676|0.004310344827586207|true|\n",
      "|wanted to check o...|  3.0|[wanted, to, chec...|0.18435754189944134| 0.01293103448275862|true|\n",
      "|came here with my...|  3.0|[came, here, with...|0.11173184357541899| 0.01293103448275862|true|\n",
      "|came here for a b...|  3.0|[came, here, for,...|0.10614525139664804|0.004310344827586207|true|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "|love coming here ...|  4.0|[love, coming, he...|0.15083798882681565|0.008620689655172414|true|\n",
      "|had their chocola...|  4.0|[had, their, choc...| 0.1005586592178771|0.008620689655172414|true|\n",
      "|who would have gu...|  4.0|[who, would, have...| 0.1452513966480447|0.008620689655172414|true|\n",
      "|this is currently...|  4.0|[this, is, curren...| 0.1005586592178771|0.008620689655172414|true|\n",
      "|a must stop when ...|  4.0|[must, stop, when...|0.11173184357541899|0.017241379310344827|true|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+\n",
      "|                text|label|               words|           ratio_eng|           ratio_ger| Eng|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+\n",
      "|super simple plac...|  5.0|[super, simple, p...|0.055865921787709494|0.004310344827586207|true|\n",
      "|small unassuming ...|  5.0|[small, unassumin...| 0.12849162011173185|0.008620689655172414|true|\n",
      "|lester s is locat...|  5.0|[lester, is, loca...| 0.13966480446927373|0.004310344827586207|true|\n",
      "|this place is awe...|  5.0|[this, place, is,...|  0.1005586592178771|                 0.0|true|\n",
      "|this place is rea...|  5.0|[this, place, is,...|  0.0670391061452514|0.008620689655172414|true|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_star_df2 = sw_and_stemming(one_star_df1)\n",
    "one_star_df2.show(5)\n",
    "\n",
    "two_star_df2 = sw_and_stemming(two_star_df1)\n",
    "two_star_df2.show(5)\n",
    "\n",
    "three_star_df2 = sw_and_stemming(three_star_df1)\n",
    "three_star_df2.show(5)\n",
    "\n",
    "four_star_df2 = sw_and_stemming(four_star_df1)\n",
    "four_star_df2.show(5)\n",
    "\n",
    "five_star_df2 = sw_and_stemming(five_star_df1)\n",
    "five_star_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def sw_and_stemming_removed(df):\n",
    "    more_stopwords = ['', 'com', 'de', 'eu', 'cf', 'pm', 'like', 'one', 'using', 'new', 'also',\n",
    "         'really', 'need', 'caption', 'since', 'change', 'young', 'align', 'width',\n",
    "         'attachment', 'number', 'know', 'two', 'use', 'see', 'get', 'first', 'good',\n",
    "         'next', 'well', 'day', 'way', 'fruit', 'different', 'let', 'lot', 'would',\n",
    "         'already', 'set', 'user', 'even', 'might', 'many', 'different', 'crazy',\n",
    "         'may', 'could', 'still', 'probably', 'make', 'write', 'used', 'written',\n",
    "         'go', 'us', 'yes', 'seen', 'behind', 'much', 'makes', 'via', 'based',\n",
    "         'choose', 'presented', 'away', 'hence', 'wants', 'please', 'add',\n",
    "         'something', 'conclusion', 'able', 'describe', 'thing', 'likely',\n",
    "         'lots', 'sense', 'higher', 'every', 'right', 'sure', 'quite', 'without']\n",
    "    \n",
    "    tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", gaps=True, pattern=r'\\s+', minTokenLength=2)\n",
    "\n",
    "    stopword_Remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "    stopword_Remover.setStopWords(stopword_Remover.getStopWords() + more_stopwords)\n",
    "\n",
    "    final_tokens_df = stopword_Remover.transform(df)\n",
    "\n",
    "    # Stemming\n",
    "    stem_words = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "    udf_Stemmed = udf(lambda l: [stem_words.stem(s) for s in l], ArrayType(StringType()))\n",
    "\n",
    "    new_final_tokens_df = final_tokens_df.withColumn(\"filteredStemmed\", udf_Stemmed(final_tokens_df[\"filtered\"]))\n",
    "    \n",
    "    return new_final_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|            filtered|     filteredStemmed|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|food is very blan...|  1.0|[food, is, very, ...| 0.0670391061452514|0.004310344827586207|true|[food, bland, aut...|[food, bland, aut...|\n",
      "|if you have not y...|  1.0|[if, you, have, n...|0.13966480446927373|0.004310344827586207|true|[yet, tried, wasa...|[yet, tri, wasabi...|\n",
      "|worse customer se...|  1.0|[worse, customer,...| 0.0670391061452514|0.004310344827586207|true|[worse, customer,...|[wors, custom, se...|\n",
      "|after reading the...|  1.0|[after, reading, ...| 0.1787709497206704| 0.01293103448275862|true|[reading, reviews...|[read, review, ye...|\n",
      "|came here for a f...|  1.0|[came, here, for,...| 0.2346368715083799|0.017241379310344827|true|[came, friend, bi...|[came, friend, bi...|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|            filtered|     filteredStemmed|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|a few years ago  ...|  2.0|[few, years, ago,...|0.07262569832402235|0.008620689655172414|true|[years, ago, plac...|[year, ago, place...|\n",
      "|atmosphere for th...|  2.0|[atmosphere, for,...|0.22346368715083798|0.017241379310344827|true|[atmosphere, rest...|[atmospher, resta...|\n",
      "|decided to give t...|  2.0|[decided, to, giv...| 0.1005586592178771|0.004310344827586207|true|[decided, give, p...|[decid, give, pla...|\n",
      "|you can see that ...|  2.0|[you, can, see, t...|0.12849162011173185|0.004310344827586207|true|[people, eat, vou...|[peopl, eat, vouc...|\n",
      "|i think i may hav...|  2.0|[think, may, have...|0.25139664804469275|0.017241379310344827|true|[think, place, ra...|[think, place, ra...|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|            filtered|     filteredStemmed|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|not bad   love th...|  3.0|[not, bad, love, ...|0.08379888268156424|0.004310344827586207|true|[bad, love, glute...|[bad, love, glute...|\n",
      "|server was a litt...|  3.0|[server, was, lit...| 0.0446927374301676|0.004310344827586207|true|[server, little, ...|[server, littl, r...|\n",
      "|wanted to check o...|  3.0|[wanted, to, chec...|0.18435754189944134| 0.01293103448275862|true|[wanted, check, p...|[want, check, pla...|\n",
      "|came here with my...|  3.0|[came, here, with...|0.11173184357541899| 0.01293103448275862|true|[came, girlfriend...|[came, girlfriend...|\n",
      "|came here for a b...|  3.0|[came, here, for,...|0.10614525139664804|0.004310344827586207|true|[came, burger, fr...|[came, burger, fr...|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|                text|label|               words|          ratio_eng|           ratio_ger| Eng|            filtered|     filteredStemmed|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "|love coming here ...|  4.0|[love, coming, he...|0.15083798882681565|0.008620689655172414|true|[love, coming, pl...|[love, come, plac...|\n",
      "|had their chocola...|  4.0|[had, their, choc...| 0.1005586592178771|0.008620689655172414|true|[chocolate, almon...|[chocol, almond, ...|\n",
      "|who would have gu...|  4.0|[who, would, have...| 0.1452513966480447|0.008620689655172414|true|[guess, fairly, d...|[guess, fair, dec...|\n",
      "|this is currently...|  4.0|[this, is, curren...| 0.1005586592178771|0.008620689655172414|true|[currently, paren...|[current, parent,...|\n",
      "|a must stop when ...|  4.0|[must, stop, when...|0.11173184357541899|0.017241379310344827|true|[must, stop, mont...|[must, stop, mont...|\n",
      "+--------------------+-----+--------------------+-------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+--------------------+--------------------+\n",
      "|                text|label|               words|           ratio_eng|           ratio_ger| Eng|            filtered|     filteredStemmed|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+--------------------+--------------------+\n",
      "|super simple plac...|  5.0|[super, simple, p...|0.055865921787709494|0.004310344827586207|true|[super, simple, p...|[super, simpl, pl...|\n",
      "|small unassuming ...|  5.0|[small, unassumin...| 0.12849162011173185|0.008620689655172414|true|[small, unassumin...|[small, unassum, ...|\n",
      "|lester s is locat...|  5.0|[lester, is, loca...| 0.13966480446927373|0.004310344827586207|true|[lester, located,...|[lester, locat, b...|\n",
      "|this place is awe...|  5.0|[this, place, is,...|  0.1005586592178771|                 0.0|true|[place, awesome, ...|[place, awesom, d...|\n",
      "|this place is rea...|  5.0|[this, place, is,...|  0.0670391061452514|0.008620689655172414|true|[place, best, res...|[place, best, res...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_star_df3 = sw_and_stemming_removed(one_star_df2)\n",
    "one_star_df3.show(5)\n",
    "\n",
    "two_star_df3 = sw_and_stemming_removed(two_star_df2)\n",
    "two_star_df3.show(5)\n",
    "\n",
    "three_star_df3 = sw_and_stemming_removed(three_star_df2)\n",
    "three_star_df3.show(5)\n",
    "\n",
    "four_star_df3 = sw_and_stemming_removed(four_star_df2)\n",
    "four_star_df3.show(5)\n",
    "\n",
    "five_star_df3 = sw_and_stemming_removed(five_star_df2)\n",
    "five_star_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:49470)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\", line 958, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vaibh\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\", line 1096, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:49470)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1095\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-513c2fb76e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add ID column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mone_star_df3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_star_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtwo_star_df3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwo_star_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mthree_star_df3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthree_star_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\pyspark\\sql\\functions.py\u001b[0m in \u001b[0;36mmonotonically_increasing_id\u001b[1;34m()\u001b[0m\n\u001b[0;32m    564\u001b[0m     \"\"\"\n\u001b[0;32m    565\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1676\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1678\u001b[1;33m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[0;32m   1679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1010\u001b[0m          \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \"\"\"\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    964\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    965\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1106\u001b[0m                 \u001b[1;34m\"server ({0}:{1})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:49470)"
     ]
    }
   ],
   "source": [
    "# Add ID column\n",
    "id=mi()\n",
    "one_star_df3 = one_star_df3.withColumn('ID', id)\n",
    "two_star_df3 = two_star_df3.withColumn('ID', id)\n",
    "three_star_df3 = three_star_df3.withColumn('ID', id)\n",
    "four_star_df3 = four_star_df3.withColumn('ID', id)\n",
    "five_star_df3 = five_star_df3.withColumn('ID', id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "def create_feature_vectors(df):\n",
    "    cv = CountVectorizer(inputCol=\"filteredStemmed\", outputCol=\"features\",\n",
    "                         vocabSize=50000,\n",
    "                         minTF=2, # of words that must appear in a document\n",
    "                         minDF=4) # of documents a word must appear show up in\n",
    "\n",
    "    cv_Model = cv.fit(df)\n",
    "\n",
    "    count_Vectors = (cv_Model.transform(df).select(\"ID\", \"features\").cache())\n",
    "    \n",
    "    return count_Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_df4 = create_feature_vectors(one_star_df3)\n",
    "\n",
    "two_star_df4 = create_feature_vectors(two_star_df3)\n",
    "\n",
    "three_star_df4 = create_feature_vectors(three_star_df3)\n",
    "\n",
    "four_star_df4 = create_feature_vectors(four_star_df3)\n",
    "\n",
    "five_star_df4 = create_feature_vectors(five_star_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: 6.1589394814669784,6.4188039262149035\n",
      "+-----------------------------------------------------------------------------+\n",
      "|words                                                                        |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|[food, place, restaur, better, bad, review, great, qualiti, servic, got]     |\n",
      "|[wait, tabl, minut, drink, ask, restaur, seat, came, hour, server]           |\n",
      "|[room, wine, glass, dirti, door, clean, park, hotel, car, floor]             |\n",
      "|[price, shrimp, worst, ice, dinner, pho, cream, game, pretti, cours]         |\n",
      "|[salad, never, sushi, came, didn, bad, roll, cold, horribl, food]            |\n",
      "|[servic, custom, server, bartend, ever, manag, tip, bill, pay, bar]          |\n",
      "|[lunch, breakfast, store, lobster, card, coupon, cake, stop, pie, red]       |\n",
      "|[burger, eat, burrito, friend, fri, free, wasn, fresh, salti, bun]           |\n",
      "|[meal, menu, potato, noth, special, small, aw, ok, extrem, portion]          |\n",
      "|[bag, la, bagel, big, le, pour, et, que, est, un]                            |\n",
      "|[charg, guy, dish, item, hand, hair, offer, option, salt, extra]             |\n",
      "|[order, got, ask, egg, water, take, coffe, back, minut, wrong]               |\n",
      "|[experi, onion, veggi, ring, morn, pickl, make, doubl, general, omelett]     |\n",
      "|[sale, slam, veteran, exchang, kebab, associ, outlet, cafeteria, manner, rob]|\n",
      "|[pizza, order, chees, steak, cook, wing, deliveri, bread, top, slice]        |\n",
      "|[reserv, husband, parti, made, daughter, dollar, show, sign, told, person]   |\n",
      "|[buffet, rice, meat, bar, beef, place, hot, restaur, crab, piec]             |\n",
      "|[taco, min, beer, fish, ask, come, bean, chip, salsa, cup]                   |\n",
      "|[chicken, fri, tast, sandwich, sauc, drive, soup, noodl, close, flavor]      |\n",
      "|[time, call, locat, manag, back, ve, owner, said, never, rude]               |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|weights                                                                         |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[0.3604, 0.1662, 0.0218, 0.0168, 0.0153, 0.0141, 0.0129, 0.0119, 0.0103, 0.0094]|\n",
      "|[0.0706, 0.0636, 0.0560, 0.0420, 0.0306, 0.0265, 0.0230, 0.0227, 0.0217, 0.0214]|\n",
      "|[0.0943, 0.0658, 0.0604, 0.0585, 0.0532, 0.0470, 0.0425, 0.0399, 0.0333, 0.0275]|\n",
      "|[0.1064, 0.0694, 0.0555, 0.0424, 0.0333, 0.0318, 0.0279, 0.0263, 0.0243, 0.0148]|\n",
      "|[0.0361, 0.0289, 0.0272, 0.0247, 0.0234, 0.0231, 0.0207, 0.0205, 0.0199, 0.0166]|\n",
      "|[0.1379, 0.0617, 0.0252, 0.0203, 0.0200, 0.0198, 0.0196, 0.0196, 0.0173, 0.0158]|\n",
      "|[0.1083, 0.0783, 0.0451, 0.0428, 0.0394, 0.0393, 0.0346, 0.0247, 0.0202, 0.0171]|\n",
      "|[0.1578, 0.1376, 0.0456, 0.0424, 0.0376, 0.0280, 0.0249, 0.0226, 0.0162, 0.0141]|\n",
      "|[0.1564, 0.1367, 0.0473, 0.0460, 0.0453, 0.0371, 0.0341, 0.0239, 0.0230, 0.0213]|\n",
      "|[0.0474, 0.0438, 0.0426, 0.0297, 0.0256, 0.0239, 0.0230, 0.0212, 0.0183, 0.0182]|\n",
      "|[0.0506, 0.0473, 0.0345, 0.0343, 0.0316, 0.0273, 0.0255, 0.0151, 0.0138, 0.0135]|\n",
      "|[0.3622, 0.0333, 0.0261, 0.0227, 0.0224, 0.0221, 0.0192, 0.0189, 0.0165, 0.0147]|\n",
      "|[0.2985, 0.0831, 0.0434, 0.0301, 0.0232, 0.0224, 0.0221, 0.0183, 0.0167, 0.0154]|\n",
      "|[0.0169, 0.0165, 0.0122, 0.0115, 0.0113, 0.0106, 0.0105, 0.0102, 0.0098, 0.0087]|\n",
      "|[0.1421, 0.0794, 0.0601, 0.0530, 0.0394, 0.0378, 0.0229, 0.0193, 0.0141, 0.0121]|\n",
      "|[0.0731, 0.0594, 0.0414, 0.0339, 0.0189, 0.0181, 0.0178, 0.0168, 0.0131, 0.0130]|\n",
      "|[0.0572, 0.0469, 0.0436, 0.0357, 0.0278, 0.0269, 0.0261, 0.0224, 0.0220, 0.0205]|\n",
      "|[0.0966, 0.0904, 0.0685, 0.0544, 0.0451, 0.0409, 0.0317, 0.0302, 0.0287, 0.0169]|\n",
      "|[0.1301, 0.0535, 0.0534, 0.0527, 0.0365, 0.0222, 0.0193, 0.0187, 0.0181, 0.0180]|\n",
      "|[0.1543, 0.0433, 0.0382, 0.0357, 0.0314, 0.0208, 0.0177, 0.0168, 0.0148, 0.0147]|\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA Model - One Star Rating\n",
    "train_data_df, test_data_df = one_star_df4.randomSplit([0.8, 0.2], 1)\n",
    "# print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "numTopics = 20 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True, maxIter = 50, \n",
    "          learningDecay = 0.51,  learningOffset = 64.0, subsamplingRate = 0.05)\n",
    "\n",
    "lda_Model = lda.fit(train_data_df)\n",
    "log_prex_test, log_prex_train = lda_Model.logPerplexity(test_data_df), lda_Model.logPerplexity(train_data_df)\n",
    "print(\"Perplexity of Test Dataset and Train Dataset: \" + str(log_prex_train) + ',' + str(log_prex_test))\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=10)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]))\n",
    "FormatNumbers = udf(lambda y: [\"{:1.4f}\".format(x) for x in y])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights_of_words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+----------------------------------------+\n",
      "|topic|words                                   |weights                                 |\n",
      "+-----+----------------------------------------+----------------------------------------+\n",
      "|1    |[food, place, restaur, better, bad]     |[0.3604, 0.1662, 0.0218, 0.0168, 0.0153]|\n",
      "|2    |[wait, tabl, minut, drink, ask]         |[0.0706, 0.0636, 0.056, 0.042, 0.0306]  |\n",
      "|3    |[room, wine, glass, dirti, door]        |[0.0943, 0.0658, 0.0604, 0.0585, 0.0532]|\n",
      "|4    |[price, shrimp, worst, ice, dinner]     |[0.1064, 0.0694, 0.0555, 0.0424, 0.0333]|\n",
      "|5    |[salad, never, sushi, came, didn]       |[0.0361, 0.0289, 0.0272, 0.0247, 0.0234]|\n",
      "|6    |[servic, custom, server, bartend, ever] |[0.1379, 0.0617, 0.0252, 0.0203, 0.02]  |\n",
      "|7    |[lunch, breakfast, store, lobster, card]|[0.1083, 0.0783, 0.0451, 0.0428, 0.0394]|\n",
      "|8    |[burger, eat, burrito, friend, fri]     |[0.1578, 0.1376, 0.0456, 0.0424, 0.0376]|\n",
      "|9    |[meal, menu, potato, noth, special]     |[0.1564, 0.1367, 0.0473, 0.046, 0.0453] |\n",
      "|10   |[bag, la, bagel, big, le]               |[0.0474, 0.0438, 0.0426, 0.0297, 0.0256]|\n",
      "|11   |[charg, guy, dish, item, hand]          |[0.0506, 0.0473, 0.0345, 0.0343, 0.0316]|\n",
      "|12   |[order, got, ask, egg, water]           |[0.3622, 0.0333, 0.0261, 0.0227, 0.0224]|\n",
      "|13   |[experi, onion, veggi, ring, morn]      |[0.2985, 0.0831, 0.0434, 0.0301, 0.0232]|\n",
      "|14   |[sale, slam, veteran, exchang, kebab]   |[0.0169, 0.0165, 0.0122, 0.0115, 0.0113]|\n",
      "|15   |[pizza, order, chees, steak, cook]      |[0.1421, 0.0794, 0.0601, 0.053, 0.0394] |\n",
      "|16   |[reserv, husband, parti, made, daughter]|[0.0731, 0.0594, 0.0414, 0.0339, 0.0189]|\n",
      "|17   |[buffet, rice, meat, bar, beef]         |[0.0572, 0.0469, 0.0436, 0.0357, 0.0278]|\n",
      "|18   |[taco, min, beer, fish, ask]            |[0.0966, 0.0904, 0.0685, 0.0544, 0.0451]|\n",
      "|19   |[chicken, fri, tast, sandwich, sauc]    |[0.1301, 0.0535, 0.0534, 0.0527, 0.0365]|\n",
      "|20   |[time, call, locat, manag, back]        |[0.1543, 0.0433, 0.0382, 0.0357, 0.0314]|\n",
      "+-----+----------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 22345\n"
     ]
    }
   ],
   "source": [
    "# One Star Rating Topic Results\n",
    "\n",
    "from pyspark.sql.types import StructType, FloatType\n",
    "maxTermsPerTopic = 5\n",
    "\n",
    "# Print the topics, showing the top-weighted terms for each topic.\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=maxTermsPerTopic)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "numTopics = topics.count()\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]), ArrayType(StringType()))\n",
    "FormatNumbers = udf(lambda y: [float(\"{:1.4f}\".format(x)) for x in y], ArrayType(FloatType()))\n",
    "\n",
    "top_topics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights_of_words'))\n",
    "\n",
    "top_topics.show(truncate=False, n=numTopics)\n",
    "\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: 6.301070797376387,6.571717312813163\n",
      "+---------------------------------------------------------------------------------+\n",
      "|words                                                                            |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|[food, servic, place, eat, nice, time, order, hour, manag, told]                 |\n",
      "|[order, wait, minut, came, chicken, custom, tabl, want, back, time]              |\n",
      "|[yesterday, train, pictur, hole, spici, sound, adult, post, strong, warm]        |\n",
      "|[tast, best, yet, cheap, excit, restaur, disgust, pretti, instead, boyfriend]    |\n",
      "|[place, owner, take, anyth, order, ve, anoth, come, chees, never]                |\n",
      "|[sit, plate, experi, think, sauc, today, whole, salad, deliv, menus]             |\n",
      "|[place, disappoint, left, go, long, guy, min, minut, full, honest]               |\n",
      "|[friend, bad, brought, door, thing, bland, finish, forev, busi, hour]            |\n",
      "|[leav, someon, stay, order, read, serious, anyon, free, extrem, other]           |\n",
      "|[ok, sat, grab, parti, dessert, immedi, ramen, pepper, re, black]                |\n",
      "|[believ, websit, oh, street, cut, suck, imagin, far, wave, prefer]               |\n",
      "|[outsid, sub, room, product, pain, par, basket, pathet, enchilada, salti]        |\n",
      "|[bite, quick, wife, understand, miss, soggi, stale, amount, doubl, combo]        |\n",
      "|[deal, bag, runni, entertain, whoever, treatment, costco, concert, util, easiest]|\n",
      "|[look, chef, averag, break, lack, window, local, cater, china, absolut]          |\n",
      "|[time, last, roll, dish, told, check, sushi, return, recommend, best]            |\n",
      "|[star, went, want, fish, definit, option, famili, doesn, bowl, won]              |\n",
      "|[around, gave, love, coupl, deliveri, plain, glass, mash, rubberi, mouth]        |\n",
      "|[worst, order, waitress, review, charg, chees, lunch, work, got, eat]            |\n",
      "|[said, ask, busi, night, start, never, burrito, slow, experi, server]            |\n",
      "+---------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|weights                                                                         |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[0.3036, 0.0662, 0.0552, 0.0226, 0.0217, 0.0198, 0.0177, 0.0129, 0.0118, 0.0117]|\n",
      "|[0.0775, 0.0454, 0.0404, 0.0352, 0.0331, 0.0299, 0.0273, 0.0215, 0.0209, 0.0196]|\n",
      "|[0.0570, 0.0564, 0.0328, 0.0292, 0.0268, 0.0250, 0.0230, 0.0212, 0.0186, 0.0176]|\n",
      "|[0.0678, 0.0595, 0.0309, 0.0305, 0.0234, 0.0228, 0.0208, 0.0205, 0.0197, 0.0189]|\n",
      "|[0.0958, 0.0316, 0.0278, 0.0268, 0.0249, 0.0231, 0.0211, 0.0192, 0.0188, 0.0175]|\n",
      "|[0.0800, 0.0591, 0.0512, 0.0502, 0.0451, 0.0387, 0.0316, 0.0279, 0.0251, 0.0209]|\n",
      "|[0.0628, 0.0461, 0.0439, 0.0237, 0.0217, 0.0213, 0.0206, 0.0175, 0.0164, 0.0148]|\n",
      "|[0.1282, 0.0851, 0.0377, 0.0313, 0.0305, 0.0273, 0.0228, 0.0219, 0.0188, 0.0175]|\n",
      "|[0.1033, 0.0629, 0.0474, 0.0426, 0.0357, 0.0341, 0.0316, 0.0288, 0.0158, 0.0152]|\n",
      "|[0.1706, 0.0820, 0.0736, 0.0422, 0.0418, 0.0405, 0.0294, 0.0290, 0.0221, 0.0175]|\n",
      "|[0.0589, 0.0584, 0.0579, 0.0531, 0.0416, 0.0337, 0.0239, 0.0222, 0.0221, 0.0218]|\n",
      "|[0.0716, 0.0443, 0.0379, 0.0324, 0.0308, 0.0235, 0.0212, 0.0205, 0.0200, 0.0199]|\n",
      "|[0.0734, 0.0694, 0.0605, 0.0368, 0.0332, 0.0292, 0.0215, 0.0210, 0.0202, 0.0193]|\n",
      "|[0.1143, 0.0945, 0.0342, 0.0316, 0.0273, 0.0180, 0.0096, 0.0093, 0.0071, 0.0068]|\n",
      "|[0.4063, 0.0417, 0.0339, 0.0313, 0.0222, 0.0140, 0.0139, 0.0121, 0.0099, 0.0093]|\n",
      "|[0.1847, 0.0456, 0.0353, 0.0335, 0.0297, 0.0290, 0.0216, 0.0209, 0.0196, 0.0170]|\n",
      "|[0.0790, 0.0728, 0.0566, 0.0523, 0.0305, 0.0296, 0.0244, 0.0162, 0.0143, 0.0121]|\n",
      "|[0.1660, 0.1250, 0.0753, 0.0713, 0.0434, 0.0321, 0.0275, 0.0212, 0.0205, 0.0168]|\n",
      "|[0.0699, 0.0603, 0.0513, 0.0422, 0.0224, 0.0140, 0.0135, 0.0122, 0.0114, 0.0103]|\n",
      "|[0.1114, 0.0749, 0.0436, 0.0389, 0.0378, 0.0267, 0.0220, 0.0190, 0.0189, 0.0181]|\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA Model - Two Star Rating\n",
    "train_data_df, test_data_df = two_star_df4.randomSplit([0.8, 0.2], 1)\n",
    "# print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "numTopics = 20 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True, maxIter = 50, \n",
    "          learningDecay = 0.51,  learningOffset = 64.0, subsamplingRate = 0.05)\n",
    "\n",
    "lda_Model = lda.fit(train_data_df)\n",
    "log_prex_test, log_prex_train = lda_Model.logPerplexity(test_data_df), lda_Model.logPerplexity(train_data_df)\n",
    "print(\"Perplexity of Test Dataset and Train Dataset: \" + str(log_prex_train) + ',' + str(log_prex_test))\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=10)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]))\n",
    "FormatNumbers = udf(lambda y: [\"{:1.4f}\".format(x) for x in y])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights_of_words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------+----------------------------------------+\n",
      "|topic|words                                  |weights                                 |\n",
      "+-----+---------------------------------------+----------------------------------------+\n",
      "|1    |[food, servic, place, eat, nice]       |[0.3036, 0.0662, 0.0552, 0.0226, 0.0217]|\n",
      "|2    |[order, wait, minut, came, chicken]    |[0.0775, 0.0454, 0.0404, 0.0352, 0.0331]|\n",
      "|3    |[yesterday, train, pictur, hole, spici]|[0.057, 0.0564, 0.0328, 0.0292, 0.0268] |\n",
      "|4    |[tast, best, yet, cheap, excit]        |[0.0678, 0.0595, 0.0309, 0.0305, 0.0234]|\n",
      "|5    |[place, owner, take, anyth, order]     |[0.0958, 0.0316, 0.0278, 0.0268, 0.0249]|\n",
      "|6    |[sit, plate, experi, think, sauc]      |[0.08, 0.0591, 0.0512, 0.0502, 0.0451]  |\n",
      "|7    |[place, disappoint, left, go, long]    |[0.0628, 0.0461, 0.0439, 0.0237, 0.0217]|\n",
      "|8    |[friend, bad, brought, door, thing]    |[0.1282, 0.0851, 0.0377, 0.0313, 0.0305]|\n",
      "|9    |[leav, someon, stay, order, read]      |[0.1033, 0.0629, 0.0474, 0.0426, 0.0357]|\n",
      "|10   |[ok, sat, grab, parti, dessert]        |[0.1706, 0.082, 0.0736, 0.0422, 0.0418] |\n",
      "|11   |[believ, websit, oh, street, cut]      |[0.0589, 0.0584, 0.0579, 0.0531, 0.0416]|\n",
      "|12   |[outsid, sub, room, product, pain]     |[0.0716, 0.0443, 0.0379, 0.0324, 0.0308]|\n",
      "|13   |[bite, quick, wife, understand, miss]  |[0.0734, 0.0694, 0.0605, 0.0368, 0.0332]|\n",
      "|14   |[deal, bag, runni, entertain, whoever] |[0.1143, 0.0945, 0.0342, 0.0316, 0.0273]|\n",
      "|15   |[look, chef, averag, break, lack]      |[0.4063, 0.0417, 0.0339, 0.0313, 0.0222]|\n",
      "|16   |[time, last, roll, dish, told]         |[0.1847, 0.0456, 0.0353, 0.0335, 0.0297]|\n",
      "|17   |[star, went, want, fish, definit]      |[0.079, 0.0728, 0.0566, 0.0523, 0.0305] |\n",
      "|18   |[around, gave, love, coupl, deliveri]  |[0.166, 0.125, 0.0753, 0.0713, 0.0434]  |\n",
      "|19   |[worst, order, waitress, review, charg]|[0.0699, 0.0603, 0.0513, 0.0422, 0.0224]|\n",
      "|20   |[said, ask, busi, night, start]        |[0.1114, 0.0749, 0.0436, 0.0389, 0.0378]|\n",
      "+-----+---------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 22345\n"
     ]
    }
   ],
   "source": [
    "# Two Star Rating Topic Results\n",
    "\n",
    "from pyspark.sql.types import StructType, FloatType\n",
    "maxTermsPerTopic = 5\n",
    "\n",
    "# Print the topics, showing the top-weighted terms for each topic.\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=maxTermsPerTopic)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "numTopics = topics.count()\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]), ArrayType(StringType()))\n",
    "FormatNumbers = udf(lambda y: [float(\"{:1.4f}\".format(x)) for x in y], ArrayType(FloatType()))\n",
    "\n",
    "top_topics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights_of_words'))\n",
    "\n",
    "top_topics.show(truncate=False, n=numTopics)\n",
    "\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: 6.364765340907587,6.618695934516439\n",
      "+---------------------------------------------------------------------------+\n",
      "|words                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|[place, food, time, went, bad, servic, bar, drink, better, never]          |\n",
      "|[spot, put, heard, sausag, nobodi, known, sister, contain, ranch, block]   |\n",
      "|[said, ok, establish, overal, anyth, child, whatev, mile, flavorless, hope]|\n",
      "|[got, peopl, eat, sit, experi, place, horribl, thought, disgust, think]    |\n",
      "|[ever, back, eat, sat, small, ve, three, dirti, call, terribl]             |\n",
      "|[order, servic, visit, night, chicken, fresh, told, pork, locat, tell]     |\n",
      "|[cook, hour, hostess, plate, insid, past, save, alway, none, son]          |\n",
      "|[worst, fri, group, order, given, place, dollar, open, disappoint, greasi] |\n",
      "|[place, anyth, expect, re, everyth, late, want, ask, els, order]           |\n",
      "|[arriv, lunch, beer, offer, took, kind, shame, tasteless, suggest, fact]   |\n",
      "|[hous, near, italian, day, agre, incorrect, avocado, twice, self, franchis]|\n",
      "|[work, happi, howev, believ, veggi, send, store, paper, garlic, pretti]    |\n",
      "|[tabl, man, weren, employe, guess, mediocr, avail, tortilla, yet, citi]    |\n",
      "|[worth, salt, pickl, piss, je, vehicl, stun, upgrad, penni, ayc]           |\n",
      "|[wait, readi, apolog, isn, tri, serious, veget, chip, can, coffe]          |\n",
      "|[didn, noth, like, slow, get, enough, st, say, server, cost]               |\n",
      "|[give, minut, manag, hard, waiter, bite, instead, today, drink, card]      |\n",
      "|[avoid, shot, street, cup, hit, kept, btw, holiday, charlott, immedi]      |\n",
      "|[food, order, look, check, server, take, restaur, time, drink, part]       |\n",
      "|[meal, gave, ask, tri, came, week, els, price, staff, rude]                |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|weights                                                                         |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[0.0472, 0.0465, 0.0352, 0.0348, 0.0347, 0.0268, 0.0199, 0.0194, 0.0193, 0.0179]|\n",
      "|[0.0495, 0.0472, 0.0464, 0.0449, 0.0390, 0.0345, 0.0298, 0.0257, 0.0217, 0.0202]|\n",
      "|[0.1838, 0.1782, 0.0350, 0.0299, 0.0174, 0.0158, 0.0155, 0.0149, 0.0145, 0.0138]|\n",
      "|[0.0928, 0.0530, 0.0328, 0.0316, 0.0292, 0.0287, 0.0212, 0.0211, 0.0211, 0.0203]|\n",
      "|[0.1272, 0.0814, 0.0753, 0.0174, 0.0172, 0.0168, 0.0165, 0.0160, 0.0140, 0.0132]|\n",
      "|[0.2107, 0.1191, 0.0533, 0.0373, 0.0361, 0.0187, 0.0172, 0.0128, 0.0126, 0.0121]|\n",
      "|[0.1046, 0.0837, 0.0305, 0.0293, 0.0283, 0.0263, 0.0213, 0.0197, 0.0164, 0.0138]|\n",
      "|[0.1491, 0.0325, 0.0241, 0.0188, 0.0185, 0.0176, 0.0158, 0.0128, 0.0126, 0.0121]|\n",
      "|[0.1130, 0.0422, 0.0320, 0.0229, 0.0189, 0.0171, 0.0156, 0.0144, 0.0144, 0.0142]|\n",
      "|[0.1056, 0.0831, 0.0770, 0.0748, 0.0434, 0.0303, 0.0242, 0.0203, 0.0175, 0.0173]|\n",
      "|[0.1428, 0.0565, 0.0565, 0.0475, 0.0355, 0.0323, 0.0232, 0.0216, 0.0214, 0.0167]|\n",
      "|[0.2331, 0.0546, 0.0489, 0.0338, 0.0317, 0.0242, 0.0212, 0.0171, 0.0142, 0.0136]|\n",
      "|[0.2986, 0.0685, 0.0531, 0.0388, 0.0315, 0.0289, 0.0193, 0.0192, 0.0154, 0.0106]|\n",
      "|[0.2597, 0.0437, 0.0207, 0.0125, 0.0107, 0.0094, 0.0081, 0.0075, 0.0071, 0.0066]|\n",
      "|[0.3080, 0.0238, 0.0168, 0.0148, 0.0135, 0.0132, 0.0127, 0.0106, 0.0094, 0.0084]|\n",
      "|[0.1854, 0.0413, 0.0206, 0.0203, 0.0199, 0.0190, 0.0165, 0.0110, 0.0105, 0.0101]|\n",
      "|[0.1250, 0.1157, 0.0779, 0.0666, 0.0412, 0.0303, 0.0293, 0.0253, 0.0184, 0.0133]|\n",
      "|[0.0276, 0.0272, 0.0254, 0.0248, 0.0247, 0.0234, 0.0221, 0.0219, 0.0180, 0.0167]|\n",
      "|[0.2264, 0.0431, 0.0284, 0.0236, 0.0228, 0.0206, 0.0195, 0.0178, 0.0174, 0.0120]|\n",
      "|[0.0594, 0.0536, 0.0432, 0.0403, 0.0370, 0.0266, 0.0253, 0.0251, 0.0230, 0.0204]|\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA Model - Three Star Rating\n",
    "train_data_df, test_data_df = three_star_df4.randomSplit([0.8, 0.2], 1)\n",
    "# print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "numTopics = 20 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True, maxIter = 50, \n",
    "          learningDecay = 0.51,  learningOffset = 64.0, subsamplingRate = 0.05)\n",
    "\n",
    "lda_Model = lda.fit(train_data_df)\n",
    "log_prex_test, log_prex_train = lda_Model.logPerplexity(test_data_df), lda_Model.logPerplexity(train_data_df)\n",
    "print(\"Perplexity of Test Dataset and Train Dataset: \" + str(log_prex_train) + ',' + str(log_prex_test))\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=10)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]))\n",
    "FormatNumbers = udf(lambda y: [\"{:1.4f}\".format(x) for x in y])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights_of_words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------+----------------------------------------+\n",
      "|topic|words                                 |weights                                 |\n",
      "+-----+--------------------------------------+----------------------------------------+\n",
      "|1    |[place, food, time, went, bad]        |[0.0472, 0.0465, 0.0352, 0.0348, 0.0347]|\n",
      "|2    |[spot, put, heard, sausag, nobodi]    |[0.0495, 0.0472, 0.0464, 0.0449, 0.039] |\n",
      "|3    |[said, ok, establish, overal, anyth]  |[0.1838, 0.1782, 0.035, 0.0299, 0.0174] |\n",
      "|4    |[got, peopl, eat, sit, experi]        |[0.0928, 0.053, 0.0328, 0.0316, 0.0292] |\n",
      "|5    |[ever, back, eat, sat, small]         |[0.1272, 0.0814, 0.0753, 0.0174, 0.0172]|\n",
      "|6    |[order, servic, visit, night, chicken]|[0.2107, 0.1191, 0.0533, 0.0373, 0.0361]|\n",
      "|7    |[cook, hour, hostess, plate, insid]   |[0.1046, 0.0837, 0.0305, 0.0293, 0.0283]|\n",
      "|8    |[worst, fri, group, order, given]     |[0.1491, 0.0325, 0.0241, 0.0188, 0.0185]|\n",
      "|9    |[place, anyth, expect, re, everyth]   |[0.113, 0.0422, 0.032, 0.0229, 0.0189]  |\n",
      "|10   |[arriv, lunch, beer, offer, took]     |[0.1056, 0.0831, 0.077, 0.0748, 0.0434] |\n",
      "|11   |[hous, near, italian, day, agre]      |[0.1428, 0.0565, 0.0565, 0.0475, 0.0355]|\n",
      "|12   |[work, happi, howev, believ, veggi]   |[0.2331, 0.0546, 0.0489, 0.0338, 0.0317]|\n",
      "|13   |[tabl, man, weren, employe, guess]    |[0.2986, 0.0685, 0.0531, 0.0388, 0.0315]|\n",
      "|14   |[worth, salt, pickl, piss, je]        |[0.2597, 0.0437, 0.0207, 0.0125, 0.0107]|\n",
      "|15   |[wait, readi, apolog, isn, tri]       |[0.308, 0.0238, 0.0168, 0.0148, 0.0135] |\n",
      "|16   |[didn, noth, like, slow, get]         |[0.1854, 0.0413, 0.0206, 0.0203, 0.0199]|\n",
      "|17   |[give, minut, manag, hard, waiter]    |[0.125, 0.1157, 0.0779, 0.0666, 0.0412] |\n",
      "|18   |[avoid, shot, street, cup, hit]       |[0.0276, 0.0272, 0.0254, 0.0248, 0.0247]|\n",
      "|19   |[food, order, look, check, server]    |[0.2264, 0.0431, 0.0284, 0.0236, 0.0228]|\n",
      "|20   |[meal, gave, ask, tri, came]          |[0.0594, 0.0536, 0.0432, 0.0403, 0.037] |\n",
      "+-----+--------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 22345\n"
     ]
    }
   ],
   "source": [
    "# Three Star Rating Topic Results\n",
    "\n",
    "from pyspark.sql.types import StructType, FloatType\n",
    "maxTermsPerTopic = 5\n",
    "\n",
    "# Print the topics, showing the top-weighted terms for each topic.\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=maxTermsPerTopic)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "numTopics = topics.count()\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]), ArrayType(StringType()))\n",
    "FormatNumbers = udf(lambda y: [float(\"{:1.4f}\".format(x)) for x in y], ArrayType(FloatType()))\n",
    "\n",
    "top_topics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights_of_words'))\n",
    "\n",
    "top_topics.show(truncate=False, n=numTopics)\n",
    "\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: 6.374076570454552,6.571016625296677\n",
      "+----------------------------------------------------------------------------------+\n",
      "|words                                                                             |\n",
      "+----------------------------------------------------------------------------------+\n",
      "|[never, bad, food, order, end, back, manag, wrong, say, want]                     |\n",
      "|[got, minut, beef, open, eat, final, long, size, time, give]                      |\n",
      "|[acknowledg, avail, spinach, guarante, fourth, btw, chain, freezer, child, turnov]|\n",
      "|[went, time, didn, peopl, give, hour, never, told, call, ok]                      |\n",
      "|[place, order, look, line, experi, night, peopl, wasn, ever, burrito]             |\n",
      "|[food, wait, pizza, chicken, back, sat, famili, check, waitress, servic]          |\n",
      "|[drink, husband, hand, noodl, mean, serv, needless, worker, toronto, lobster]     |\n",
      "|[owner, money, tip, came, eat, saw, told, everyth, salt, need]                    |\n",
      "|[meat, eat, told, read, chef, finish, breakfast, pay, anyth, fish]                |\n",
      "|[mind, share, par, aren, sister, forget, ill, mention, miser, fit]                |\n",
      "|[mess, case, togeth, woman, god, midnight, rd, uninspir, explan, cracker]         |\n",
      "|[ask, great, seat, tast, manag, overcook, care, close, door, sever]               |\n",
      "|[touch, joint, leg, especi, treat, hell, question, member, lie, met]              |\n",
      "|[miss, flavorless, bottl, respond, rip, tape, luckili, soak, doesnt, meet]        |\n",
      "|[water, top, wing, burnt, order, room, kid, cours, store, late]                   |\n",
      "|[horribl, cook, three, stay, paid, tell, big, said, hair, manag]                  |\n",
      "|[restaur, call, tabl, work, host, tri, quick, spend, eat, spoke]                  |\n",
      "|[start, side, dish, bring, choic, grill, patron, eye, heard, noth]                |\n",
      "|[servic, time, ve, custom, eat, put, bill, min, server, final]                    |\n",
      "|[worst, take, last, experi, potato, left, chicken, stop, veggi, stand]            |\n",
      "+----------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|weights                                                                         |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[0.0598, 0.0559, 0.0535, 0.0503, 0.0436, 0.0426, 0.0402, 0.0391, 0.0256, 0.0195]|\n",
      "|[0.1339, 0.0651, 0.0298, 0.0264, 0.0261, 0.0228, 0.0215, 0.0213, 0.0198, 0.0198]|\n",
      "|[0.0527, 0.0242, 0.0218, 0.0190, 0.0174, 0.0161, 0.0157, 0.0139, 0.0122, 0.0113]|\n",
      "|[0.0326, 0.0306, 0.0216, 0.0190, 0.0181, 0.0140, 0.0131, 0.0124, 0.0117, 0.0113]|\n",
      "|[0.1851, 0.1679, 0.0796, 0.0319, 0.0290, 0.0245, 0.0132, 0.0130, 0.0114, 0.0086]|\n",
      "|[0.2774, 0.0614, 0.0329, 0.0264, 0.0164, 0.0157, 0.0155, 0.0127, 0.0124, 0.0112]|\n",
      "|[0.2014, 0.0978, 0.0705, 0.0289, 0.0267, 0.0187, 0.0154, 0.0146, 0.0125, 0.0117]|\n",
      "|[0.0617, 0.0568, 0.0528, 0.0390, 0.0352, 0.0352, 0.0282, 0.0274, 0.0256, 0.0252]|\n",
      "|[0.1116, 0.0456, 0.0349, 0.0330, 0.0303, 0.0279, 0.0275, 0.0235, 0.0225, 0.0205]|\n",
      "|[0.0634, 0.0589, 0.0575, 0.0426, 0.0381, 0.0282, 0.0275, 0.0204, 0.0188, 0.0178]|\n",
      "|[0.1202, 0.0614, 0.0490, 0.0485, 0.0371, 0.0270, 0.0196, 0.0183, 0.0171, 0.0164]|\n",
      "|[0.1990, 0.1073, 0.0582, 0.0379, 0.0164, 0.0162, 0.0131, 0.0117, 0.0114, 0.0099]|\n",
      "|[0.0881, 0.0699, 0.0483, 0.0429, 0.0351, 0.0343, 0.0276, 0.0259, 0.0254, 0.0245]|\n",
      "|[0.0818, 0.0562, 0.0396, 0.0345, 0.0191, 0.0187, 0.0158, 0.0152, 0.0132, 0.0129]|\n",
      "|[0.1042, 0.0744, 0.0488, 0.0464, 0.0435, 0.0330, 0.0328, 0.0284, 0.0251, 0.0237]|\n",
      "|[0.1386, 0.0646, 0.0406, 0.0362, 0.0345, 0.0323, 0.0252, 0.0205, 0.0199, 0.0120]|\n",
      "|[0.2144, 0.0889, 0.0652, 0.0408, 0.0237, 0.0221, 0.0200, 0.0197, 0.0187, 0.0174]|\n",
      "|[0.1819, 0.1098, 0.0635, 0.0405, 0.0289, 0.0243, 0.0170, 0.0168, 0.0157, 0.0143]|\n",
      "|[0.1048, 0.0957, 0.0848, 0.0432, 0.0246, 0.0222, 0.0204, 0.0197, 0.0184, 0.0164]|\n",
      "|[0.0598, 0.0569, 0.0261, 0.0256, 0.0236, 0.0227, 0.0173, 0.0172, 0.0137, 0.0135]|\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA Model - Four Star Rating\n",
    "train_data_df, test_data_df = Four_star_df4.randomSplit([0.8, 0.2], 1)\n",
    "# print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "numTopics = 20 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True, maxIter = 50, \n",
    "          learningDecay = 0.51,  learningOffset = 64.0, subsamplingRate = 0.05)\n",
    "\n",
    "lda_Model = lda.fit(train_data_df)\n",
    "log_prex_test, log_prex_train = lda_Model.logPerplexity(test_data_df), lda_Model.logPerplexity(train_data_df)\n",
    "print(\"Perplexity of Test Dataset and Train Dataset: \" + str(log_prex_train) + ',' + str(log_prex_test))\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=10)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]))\n",
    "FormatNumbers = udf(lambda y: [\"{:1.4f}\".format(x) for x in y])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights_of_words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------+----------------------------------------+\n",
      "|topic|words                                         |weights                                 |\n",
      "+-----+----------------------------------------------+----------------------------------------+\n",
      "|1    |[never, bad, food, order, end]                |[0.0598, 0.0559, 0.0535, 0.0503, 0.0436]|\n",
      "|2    |[got, minut, beef, open, eat]                 |[0.1339, 0.0651, 0.0298, 0.0264, 0.0261]|\n",
      "|3    |[acknowledg, avail, spinach, guarante, fourth]|[0.0527, 0.0242, 0.0218, 0.019, 0.0174] |\n",
      "|4    |[went, time, didn, peopl, give]               |[0.0326, 0.0306, 0.0216, 0.019, 0.0181] |\n",
      "|5    |[place, order, look, line, experi]            |[0.1851, 0.1679, 0.0796, 0.0319, 0.029] |\n",
      "|6    |[food, wait, pizza, chicken, back]            |[0.2774, 0.0614, 0.0329, 0.0264, 0.0164]|\n",
      "|7    |[drink, husband, hand, noodl, mean]           |[0.2014, 0.0978, 0.0705, 0.0289, 0.0267]|\n",
      "|8    |[owner, money, tip, came, eat]                |[0.0617, 0.0568, 0.0528, 0.039, 0.0352] |\n",
      "|9    |[meat, eat, told, read, chef]                 |[0.1116, 0.0456, 0.0349, 0.033, 0.0303] |\n",
      "|10   |[mind, share, par, aren, sister]              |[0.0634, 0.0589, 0.0575, 0.0426, 0.0381]|\n",
      "|11   |[mess, case, togeth, woman, god]              |[0.1202, 0.0614, 0.049, 0.0485, 0.0371] |\n",
      "|12   |[ask, great, seat, tast, manag]               |[0.199, 0.1073, 0.0582, 0.0379, 0.0164] |\n",
      "|13   |[touch, joint, leg, especi, treat]            |[0.0881, 0.0699, 0.0483, 0.0429, 0.0351]|\n",
      "|14   |[miss, flavorless, bottl, respond, rip]       |[0.0818, 0.0562, 0.0396, 0.0345, 0.0191]|\n",
      "|15   |[water, top, wing, burnt, order]              |[0.1042, 0.0744, 0.0488, 0.0464, 0.0435]|\n",
      "|16   |[horribl, cook, three, stay, paid]            |[0.1386, 0.0646, 0.0406, 0.0362, 0.0345]|\n",
      "|17   |[restaur, call, tabl, work, host]             |[0.2144, 0.0889, 0.0652, 0.0408, 0.0237]|\n",
      "|18   |[start, side, dish, bring, choic]             |[0.1819, 0.1098, 0.0635, 0.0405, 0.0289]|\n",
      "|19   |[servic, time, ve, custom, eat]               |[0.1048, 0.0957, 0.0848, 0.0432, 0.0246]|\n",
      "|20   |[worst, take, last, experi, potato]           |[0.0598, 0.0569, 0.0261, 0.0256, 0.0236]|\n",
      "+-----+----------------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 22345\n"
     ]
    }
   ],
   "source": [
    "# Four Star Rating Topic Results\n",
    "\n",
    "from pyspark.sql.types import StructType, FloatType\n",
    "maxTermsPerTopic = 5\n",
    "\n",
    "# Print the topics, showing the top-weighted terms for each topic.\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=maxTermsPerTopic)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "numTopics = topics.count()\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]), ArrayType(StringType()))\n",
    "FormatNumbers = udf(lambda y: [float(\"{:1.4f}\".format(x)) for x in y], ArrayType(FloatType()))\n",
    "\n",
    "top_topics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights_of_words'))\n",
    "\n",
    "top_topics.show(truncate=False, n=numTopics)\n",
    "\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: 6.237907148346874,6.451044403333594\n",
      "+-------------------------------------------------------------------------------+\n",
      "|words                                                                          |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[food, fish, bad, got, time, wast, told, say, find, credit]                    |\n",
      "|[server, didn, waitress, chees, put, hour, mind, last, eaten, save]            |\n",
      "|[particular, argu, figur, stupid, agre, laugh, along, king, throw, worker]     |\n",
      "|[locat, walk, dinner, ever, took, want, chicken, around, happen, poor]         |\n",
      "|[restaur, said, fri, night, actual, eat, tasteless, made, sorri, given]        |\n",
      "|[place, friend, seem, els, time, want, dish, water, return, check]             |\n",
      "|[meal, expens, menu, least, ten, game, dine, feel, believ, overal]             |\n",
      "|[order, tabl, serv, hope, never, dinner, felt, warm, eat, min]                 |\n",
      "|[look, sauc, seat, sandwich, bill, item, read, white, stale, greasi]           |\n",
      "|[charg, contact, indian, orang, ruin, scene, wine, god, origin, regist]        |\n",
      "|[extra, rest, bone, paper, gluten, will, learn, substitut, thru, concern]      |\n",
      "|[minut, better, experi, went, long, recommend, bar, burger, littl, ll]         |\n",
      "|[review, taco, turn, cashier, anyon, bartend, finish, person, owner, hello]    |\n",
      "|[product, desk, disabl, cajun, ride, urg, messi, calm, math, isnt]             |\n",
      "|[absolut, pick, correct, neither, coke, quick, restroom, benedict, pleas, milk]|\n",
      "|[come, wait, custom, guy, ve, time, sat, tast, slice, didn]                    |\n",
      "|[ask, go, call, busi, work, give, rice, cook, side, worst]                     |\n",
      "|[overpr, cold, option, gave, note, spot, whatev, perfect, let, la]             |\n",
      "|[back, think, disgust, take, sat, came, horribl, great, egg, big]              |\n",
      "|[servic, got, never, tri, peopl, anoth, price, take, final, show]              |\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|weights                                                                         |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[0.4218, 0.0311, 0.0280, 0.0250, 0.0244, 0.0202, 0.0186, 0.0164, 0.0160, 0.0143]|\n",
      "|[0.2102, 0.1339, 0.0912, 0.0479, 0.0239, 0.0225, 0.0222, 0.0193, 0.0154, 0.0137]|\n",
      "|[0.1112, 0.0588, 0.0569, 0.0342, 0.0334, 0.0285, 0.0281, 0.0243, 0.0239, 0.0217]|\n",
      "|[0.0322, 0.0279, 0.0279, 0.0270, 0.0210, 0.0202, 0.0169, 0.0153, 0.0152, 0.0150]|\n",
      "|[0.1256, 0.1200, 0.1063, 0.0643, 0.0381, 0.0379, 0.0196, 0.0172, 0.0155, 0.0153]|\n",
      "|[0.6169, 0.0467, 0.0328, 0.0268, 0.0246, 0.0241, 0.0208, 0.0191, 0.0110, 0.0089]|\n",
      "|[0.0750, 0.0424, 0.0375, 0.0298, 0.0218, 0.0196, 0.0172, 0.0169, 0.0161, 0.0157]|\n",
      "|[0.5132, 0.1139, 0.0479, 0.0194, 0.0133, 0.0115, 0.0115, 0.0107, 0.0098, 0.0096]|\n",
      "|[0.1354, 0.1295, 0.0817, 0.0659, 0.0413, 0.0234, 0.0222, 0.0188, 0.0168, 0.0149]|\n",
      "|[0.1288, 0.0475, 0.0385, 0.0218, 0.0196, 0.0191, 0.0188, 0.0153, 0.0153, 0.0152]|\n",
      "|[0.0878, 0.0542, 0.0456, 0.0440, 0.0308, 0.0262, 0.0200, 0.0193, 0.0163, 0.0146]|\n",
      "|[0.1175, 0.0811, 0.0733, 0.0716, 0.0635, 0.0260, 0.0253, 0.0228, 0.0207, 0.0154]|\n",
      "|[0.1144, 0.0831, 0.0538, 0.0534, 0.0503, 0.0349, 0.0317, 0.0231, 0.0230, 0.0140]|\n",
      "|[0.0563, 0.0183, 0.0141, 0.0106, 0.0103, 0.0094, 0.0094, 0.0085, 0.0076, 0.0063]|\n",
      "|[0.1685, 0.0328, 0.0211, 0.0209, 0.0202, 0.0153, 0.0152, 0.0150, 0.0148, 0.0131]|\n",
      "|[0.1552, 0.1543, 0.1239, 0.0532, 0.0456, 0.0447, 0.0359, 0.0210, 0.0183, 0.0142]|\n",
      "|[0.0816, 0.0257, 0.0249, 0.0188, 0.0185, 0.0156, 0.0145, 0.0141, 0.0129, 0.0122]|\n",
      "|[0.0657, 0.0625, 0.0499, 0.0338, 0.0223, 0.0217, 0.0167, 0.0164, 0.0159, 0.0151]|\n",
      "|[0.2179, 0.0853, 0.0435, 0.0423, 0.0411, 0.0387, 0.0325, 0.0206, 0.0203, 0.0202]|\n",
      "|[0.1407, 0.0652, 0.0541, 0.0392, 0.0198, 0.0187, 0.0174, 0.0164, 0.0138, 0.0121]|\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA Model - Five Star Rating\n",
    "train_data_df, test_data_df = five_star_df4.randomSplit([0.8, 0.2], 1)\n",
    "# print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "numTopics = 20 # number of topics\n",
    "\n",
    "lda = LDA(k = numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True, maxIter = 50, \n",
    "          learningDecay = 0.51,  learningOffset = 64.0, subsamplingRate = 0.05)\n",
    "\n",
    "lda_Model = lda.fit(train_data_df)\n",
    "log_prex_test, log_prex_train = lda_Model.logPerplexity(test_data_df), lda_Model.logPerplexity(train_data_df)\n",
    "print(\"Perplexity of Test Dataset and Train Dataset: \" + str(log_prex_train) + ',' + str(log_prex_test))\n",
    "\n",
    "# Print topics and top-weighted terms\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=10)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]))\n",
    "FormatNumbers = udf(lambda y: [\"{:1.4f}\".format(x) for x in y])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights_of_words')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------+----------------------------------------+\n",
      "|topic|words                                  |weights                                 |\n",
      "+-----+---------------------------------------+----------------------------------------+\n",
      "|1    |[food, fish, bad, got, time]           |[0.4218, 0.0311, 0.028, 0.025, 0.0244]  |\n",
      "|2    |[server, didn, waitress, chees, put]   |[0.2102, 0.1339, 0.0912, 0.0479, 0.0239]|\n",
      "|3    |[particular, argu, figur, stupid, agre]|[0.1112, 0.0588, 0.0569, 0.0342, 0.0334]|\n",
      "|4    |[locat, walk, dinner, ever, took]      |[0.0322, 0.0279, 0.0279, 0.027, 0.021]  |\n",
      "|5    |[restaur, said, fri, night, actual]    |[0.1256, 0.12, 0.1063, 0.0643, 0.0381]  |\n",
      "|6    |[place, friend, seem, els, time]       |[0.6169, 0.0467, 0.0328, 0.0268, 0.0246]|\n",
      "|7    |[meal, expens, menu, least, ten]       |[0.075, 0.0424, 0.0375, 0.0298, 0.0218] |\n",
      "|8    |[order, tabl, serv, hope, never]       |[0.5132, 0.1139, 0.0479, 0.0194, 0.0133]|\n",
      "|9    |[look, sauc, seat, sandwich, bill]     |[0.1354, 0.1295, 0.0817, 0.0659, 0.0413]|\n",
      "|10   |[charg, contact, indian, orang, ruin]  |[0.1288, 0.0475, 0.0385, 0.0218, 0.0196]|\n",
      "|11   |[extra, rest, bone, paper, gluten]     |[0.0878, 0.0542, 0.0456, 0.044, 0.0308] |\n",
      "|12   |[minut, better, experi, went, long]    |[0.1175, 0.0811, 0.0733, 0.0716, 0.0635]|\n",
      "|13   |[review, taco, turn, cashier, anyon]   |[0.1144, 0.0831, 0.0538, 0.0534, 0.0503]|\n",
      "|14   |[product, desk, disabl, cajun, ride]   |[0.0563, 0.0183, 0.0141, 0.0106, 0.0103]|\n",
      "|15   |[absolut, pick, correct, neither, coke]|[0.1685, 0.0328, 0.0211, 0.0209, 0.0202]|\n",
      "|16   |[come, wait, custom, guy, ve]          |[0.1552, 0.1543, 0.1239, 0.0532, 0.0456]|\n",
      "|17   |[ask, go, call, busi, work]            |[0.0816, 0.0257, 0.0249, 0.0188, 0.0185]|\n",
      "|18   |[overpr, cold, option, gave, note]     |[0.0657, 0.0625, 0.0499, 0.0338, 0.0223]|\n",
      "|19   |[back, think, disgust, take, sat]      |[0.2179, 0.0853, 0.0435, 0.0423, 0.0411]|\n",
      "|20   |[servic, got, never, tri, peopl]       |[0.1407, 0.0652, 0.0541, 0.0392, 0.0198]|\n",
      "+-----+---------------------------------------+----------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 22345\n"
     ]
    }
   ],
   "source": [
    "# Five Star Rating Topic Results\n",
    "\n",
    "from pyspark.sql.types import StructType, FloatType\n",
    "maxTermsPerTopic = 5\n",
    "\n",
    "# Print the topics, showing the top-weighted terms for each topic.\n",
    "topics = lda_Model.describeTopics(maxTermsPerTopic=maxTermsPerTopic)\n",
    "vocabArray = cv_Model.vocabulary\n",
    "numTopics = topics.count()\n",
    "\n",
    "ListOfIndexToWords = udf(lambda z: list([vocabArray[w] for w in z]), ArrayType(StringType()))\n",
    "FormatNumbers = udf(lambda y: [float(\"{:1.4f}\".format(x)) for x in y], ArrayType(FloatType()))\n",
    "\n",
    "top_topics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights_of_words'))\n",
    "\n",
    "top_topics.show(truncate=False, n=numTopics)\n",
    "\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
